{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "基与字符级RNN（Char-RNN）的人名生成\n",
    "*******************************************\n",
    "**作者**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n",
    "\n",
    "在 :doc:`上一个教程 </intermediate/char_rnn_classification_tutorial>` \n",
    "里我们使用RNN把名字分类到它所属的语言中, 这次我们改变一下来学习从语言中生成名字. \n",
    "\n",
    "::\n",
    "\n",
    "    > python sample.py Russian RUS\n",
    "    Rovakov\n",
    "    Uantov\n",
    "    Shavakov\n",
    "\n",
    "    > python sample.py German GER\n",
    "    Gerren\n",
    "    Ereng\n",
    "    Rosher\n",
    "\n",
    "    > python sample.py Spanish SPA\n",
    "    Salla\n",
    "    Parer\n",
    "    Allan\n",
    "\n",
    "    > python sample.py Chinese CHI\n",
    "    Chan\n",
    "    Hang\n",
    "    Iun\n",
    "\n",
    "我们仍然手工搭建一个包含几个线性层的小的RNN. 这次的最大的不同是输入一个类别, 每次输出一个字母, \n",
    "而不是读入所有名字的字母来预测一个类别. 循环的预测每一个字母来构成语言（也可以用文\n",
    "字或者其他更高级的结构完成）, 通常被称为“语言模型”. \n",
    "\n",
    "**推荐阅读: **\n",
    "\n",
    "假设你至少安装了PyTorch, 熟悉Python, 理解Tensors: \n",
    "\n",
    "-  http://pytorch.org/ : 安装说明\n",
    "-  :doc:`/beginner/deep_learning_60min_blitz` 获取一般的 PyTorch 入门\n",
    "-  :doc:`/beginner/pytorch_with_examples` 广泛且深入的概述\n",
    "-  :doc:`/beginner/former_torchies_tutorial` 如果曾经是 Lua Torch 的用户\n",
    "\n",
    "下面这些对了解 RNNs 和其工作原理也是很有用的: \n",
    "\n",
    "-  `The Unreasonable Effectiveness of Recurrent Neural\n",
    "   Networks <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>`__\n",
    "   展示了一系列真实生活中的例子\n",
    "-  `Understanding LSTM\n",
    "   Networks <http://colah.github.io/posts/2015-08-Understanding-LSTMs/>`__\n",
    "   是一篇特别关于LSTMs的文章, 但是对于一般的RNNs也很有益的\n",
    "\n",
    "还建议上一个教程:  :doc:`/intermediate/char_rnn_classification_tutorial`\n",
    "\n",
    "\n",
    "数据准备\n",
    "==================\n",
    "\n",
    ".. Note::\n",
    "   从 `这里 <https://download.pytorch.org/tutorial/data.zip>`_\n",
    "   下载数据, 并解压到当前目录. \n",
    "\n",
    "更多的细节参考上一个教程, 总之, 数据含有一批纯文本文件:  ``data/names/[Language].txt`` \n",
    "每一行一个人名. 将行分割成数组, 并把 Unicode 转换成 ASCII 编码, 最后放进一个字典里 ``{language: [names ...]}``. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# categories: 18 ['Vietnamese', 'German', 'Dutch', 'Scottish', 'Korean', 'Russian', 'Japanese', 'Czech', 'English', 'Polish', 'Arabic', 'Chinese', 'French', 'Spanish', 'Italian', 'Portuguese', 'Greek', 'Irish']\n",
      "O'Neal\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters) + 1 # 添加 EOS 标记 #59\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# 将 Unicode 字符串转换为纯 ASCII 编码, 感谢 http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# 读取文件并分割成行\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# 构建映射字典 category_lines , 每个类别是由很多个行组成的list\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = filename.split('/')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "print('# categories:', n_categories, all_categories)\n",
    "print(unicodeToAscii(\"O'Néàl\"))\n",
    "# print(category_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建网络\n",
    "====================\n",
    "\n",
    " 这个网络扩展了 `上一个教程的RNN <http://pytorch.apachecn.org/cn/tutorials/intermediate/char_rnn_classification_tutorial.html>`__ , 为类别张量添加了一个额外的参数, 并和其他的参数串联在一起. 类别张量\n",
    "和字母的输入一样是 one-hot 向量. \n",
    "\n",
    "我们将输出解释成为下一个字母的概率, 采样的时候, 最有可能的输出被当做下一个输入. \n",
    "\n",
    "为了让网络更加有效工作, 我添加了第二个线性层 ``o2o`` （在合并了隐藏层和输出层的后面）. \n",
    "还有一个 Dropout 层,  `使输入的部分值以给定的概率值随机的变成 0 <https://arxiv.org/abs/1207.0580>`__ \n",
    "（这里概率取0.1）, 这样做通常是为了模糊输入以防止过拟合. 这里我们在网络的最末端使用它, 从而故意添加一些混乱和增加采样的多样化. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://i.imgur.com/jzVrf7f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        #其实就相当于一个更深的RNN（相当于一个两层的RNN）\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        #Dropout 层, 使输入的部分值以给定的概率值随机的变成 0 （这里概率取0.1）, \n",
    "        #这样做通常是为了模糊输入以防止过拟合。这里我们在网络的最末端使用它, 从而故\n",
    "        #意添加一些混乱和增加采样的多样化.\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, category, input, hidden):\n",
    "        input_combined = torch.cat((category, input, hidden), 1)#按行进行拼接\n",
    "        hidden = self.i2h(input_combined)#计算新的隐藏层值\n",
    "        output = self.i2o(input_combined)#计算当前的输出值\n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练\n",
    "=========\n",
    "训练前的准备\n",
    "----------------------\n",
    "\n",
    "首先, 利用辅助函数产生随机的（category, line）对: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 从list中随机选取项\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# 获取随机的类别和该类别中随机的行\n",
    "def randomTrainingPair():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    return category, line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对每一个时间点（也就是说在训练集中词的每个字母）网络的输入是 ``(类别, 当前字母, 隐藏层状态)`` , \n",
    "输出是 ``(下一个字母, 下一个隐藏层状态)`` . 对于每一个训练集, 我们需要的是类别、输入的字母集、输出/目标字母集. \n",
    "\n",
    "因为在每一步, 我们从当前的字母预测下一个字母, 这样的字母对是在原有行中连续字母的集合, \n",
    "例如, 对于 ``\"ABCD<EOS>\"`` 将会产生 (\"A\", \"B\"), (\"B\", \"C\"),\n",
    "(\"C\", \"D\"), (\"D\", \"EOS\"). \n",
    "\n",
    ".. figure:: https://i.imgur.com/JH58tXY.png\n",
    "   :alt:\n",
    "\n",
    "类别张量是一个大小为 ``<1 x n_categories>`` 的 `one-hot\n",
    "tensor <https://en.wikipedia.org/wiki/One-hot>`__ 张量, \n",
    "在训练的每一个时间点把它喂给网络 —— 这是一个设计的选择, 它可以被当作为初始隐藏状或其他策略的一部分. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 类别的 one-hot 向量\n",
    "def categoryTensor(category):\n",
    "    li = all_categories.index(category)\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][li] = 1\n",
    "    return tensor\n",
    "\n",
    "# 输入串从第一个字母到最后一个字母（不包括 EOS ）的 one-hot 矩阵\n",
    "def inputTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# 目标的第二个字母到结尾（EOS）的 LongTensor \n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了训练过程的便利, 添加一个 ``randomTrainingExample`` 函数, 获取随机的 (category, line) 对, \n",
    "并把他们转换成需要的 (category, input, target) 张量. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 从随机的（category, line）对中生成 category, input, and target 张量 \n",
    "def randomTrainingExample():\n",
    "    category, line = randomTrainingPair()\n",
    "    category_tensor = Variable(categoryTensor(category))\n",
    "    input_line_tensor = Variable(inputTensor(line))\n",
    "    target_line_tensor = Variable(targetTensor(line))\n",
    "    return category_tensor, input_line_tensor, target_line_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络的训练\n",
    "--------------------\n",
    "\n",
    "与分类相比, 分类只用到了最后的输出, 而这里每个步都会产生一个预测, 所以我们需要计算每一步的损失. \n",
    "\n",
    "自动求导（autograd）的魔力就在于, 它允许将每一步的损失简单的加和, 并在最后调用 backward \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    target_line_tensor.size()\n",
    "    #调整纬度，否则在调用损失函数计算损失时将出现无纬度的tensor变量 \n",
    "    #target_line_tensor[i]是无纬度tensor变量\n",
    "    Target_line_tensor=target_line_tensor.reshape(target_line_tensor.size(0),1)\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(input_line_tensor.size()[0]):\n",
    "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)        \n",
    "        loss += criterion(output, Target_line_tensor[i])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item() / input_line_tensor.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了跟踪训练花费了多长时间, 这里添加一个 ``timeSince(timestamp)`` 函数, 返回一个人们易读的字符串: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练和往常一样, 不停的调用 train 并等待一会, 打印当前时间, 每隔 ``print_every`` \n",
    "个例子打印 loss, 将每 ``plot_every`` 个例子的平均损失保存在 ``all_losses`` 中以便后面画图. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (i2h): Linear(in_features=205, out_features=128, bias=True)\n",
      "  (i2o): Linear(in_features=205, out_features=59, bias=True)\n",
      "  (o2o): Linear(in_features=187, out_features=59, bias=True)\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_letters, 128, n_letters)# n_letters = 59\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 19s (5000 5%) 3.0099\n",
      "0m 36s (10000 10%) 3.1469\n",
      "0m 48s (15000 15%) 2.3738\n",
      "1m 7s (20000 20%) 2.0605\n",
      "1m 20s (25000 25%) 2.3328\n",
      "1m 39s (30000 30%) 2.2508\n",
      "1m 50s (35000 35%) 2.2017\n",
      "2m 2s (40000 40%) 3.1764\n",
      "2m 21s (45000 45%) 1.8994\n",
      "2m 34s (50000 50%) 2.4012\n",
      "2m 48s (55000 55%) 3.2996\n",
      "3m 5s (60000 60%) 2.1706\n",
      "3m 16s (65000 65%) 2.2212\n",
      "3m 28s (70000 70%) 2.4213\n",
      "3m 40s (75000 75%) 2.2987\n",
      "3m 51s (80000 80%) 2.3018\n",
      "4m 7s (85000 85%) 2.0314\n",
      "4m 19s (90000 90%) 2.5594\n",
      "4m 33s (95000 95%) 2.9566\n",
      "4m 46s (100000 100%) 1.1936\n"
     ]
    }
   ],
   "source": [
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # 每 plot_every 次迭代需要重置\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category_tensor, input_line_tensor, target_line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, input_line_tensor, target_line_tensor)\n",
    "    total_loss += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6]) tensor(0.)\n",
      "torch.Size([6, 1])\n",
      "tensor([[ 0.],\n",
      "        [17.],\n",
      "        [ 2.],\n",
      "        [14.],\n",
      "        [13.],\n",
      "        [58.]])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "#解决tensor无纬度的问题：a[0]\n",
    "a=torch.Tensor([ 0, 17,  2, 14, 13, 58])\n",
    "print(a.size(),a[0])\n",
    "b=a.reshape(6,1)\n",
    "print(b.size())\n",
    "print(b)\n",
    "print(b[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制损失\n",
    "-------------------\n",
    "\n",
    "从 all\\_losses 中绘制历史损失, 以展现网络的学习过程\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8SulVC+jiV8ppXoZTfxKKdXL/H+D3KZLNiu/KAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4fbd579358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络采样\n",
    "====================\n",
    "\n",
    "为了采样, 我们给网络一个字母并问下一个字母是什么, 重复这个过程直到 EOS 标记. \n",
    "\n",
    "-  创建输入类别、起始字母和隐藏层状态的张量\n",
    "-  创建一个带有起始字母的 ``output_name`` 串\n",
    "-  直到最大的输出长度, \n",
    "\n",
    "   -  当前字母喂给网络\n",
    "   -  从最高的输出获取下一个字母和下一个隐藏层状态\n",
    "   -  如果输出字母是 EOS, 算法结束\n",
    "   -  如果输出是常规字母, 将其加入到 ``output_name`` 并继续\n",
    "\n",
    "-  返回最终的名字\n",
    "\n",
    ".. Note::\n",
    "   与给定起始字母不同的是, 有其他的策略是在训练的时候包含一个“串起始”标记, 让网络选择属于自己的起始字母. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranakov\n",
      "Uanakov\n",
      "Sheran\n",
      "Ganter\n",
      "Ereng\n",
      "Rong\n",
      "Sanala\n",
      "Pana\n",
      "Aranan\n",
      "Chan\n",
      "Han\n",
      "Iun\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "\n",
    "# 从类别和起始字母采样\n",
    "def sample(category, start_letter='A'):\n",
    "    category_tensor = Variable(categoryTensor(category))\n",
    "    input = Variable(inputTensor(start_letter))#torch.Size([1, 1, 59])\n",
    "\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    output_name = start_letter\n",
    "\n",
    "    for i in range(max_length):\n",
    "        #output.data.size() = torch.Size([1, 59])\n",
    "        output, hidden = rnn(category_tensor, input[0], hidden)\n",
    "        topv, topi = output.data.topk(1)\n",
    "        topi = topi[0][0]\n",
    "        if topi == n_letters - 1:\n",
    "            break\n",
    "        else:\n",
    "            letter = all_letters[topi]\n",
    "            output_name += letter\n",
    "        input = Variable(inputTensor(letter))\n",
    "\n",
    "    return output_name\n",
    "\n",
    "# 给定一个类别和多个起始字母 获取个采样结果\n",
    "def samples(category, start_letters='ABC'):\n",
    "    for start_letter in start_letters:\n",
    "        print(sample(category, start_letter))\n",
    "\n",
    "samples('Russian', 'RUS')\n",
    "\n",
    "samples('German', 'GER')\n",
    "\n",
    "samples('Spanish', 'SPA')\n",
    "\n",
    "samples('Chinese', 'CHI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),)\n",
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'- 58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.where(category_lines['Russian']==['Ranakov','Uanakov','Sheran','Ganter','Ereng',\n",
    "                                           'Rong','Sanala','Pana','Aranan','Chan','Han','Iun']))\n",
    "print(all_letters,len(all_letters))\n",
    "all_letters[57]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "练习\n",
    "=========\n",
    "\n",
    "-  尝试使用不同 类别->行 数据集, 例如: \n",
    "\n",
    "   -  小说系列 -> 角色名字\n",
    "   -  词性 -> 词语\n",
    "   -  国家 -> 城市\n",
    "\n",
    "-  使用“串起始”标记, 使采样的时候不用给定起始字母\n",
    "-  使用更大和/或更好的网络结构获取更好的结果\n",
    "\n",
    "   -  尝试一下 nn.LSTM 和 nn.GRU 层\n",
    "   -  将这些 RNNs 组合成更高级的网络\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
