{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# MNIST dataset \n",
    "dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                     train=True, \n",
    "                                     transform=transforms.ToTensor(),  \n",
    "                                     download=True)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=500, num_classes=10):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (fc1): Linear(in_features=784, out_features=500, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "dummy_input = torch.rand(784) #假设输入13张1*28*28的图片\n",
    "model(dummy_input)\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with SummaryWriter(comment='NNet') as w:\n",
    "    w.add_graph(model, (dummy_input, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = Logger('./logs')\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)  \n",
    "\n",
    "data_iter = iter(data_loader)\n",
    "# print(data_iter)\n",
    "iter_per_epoch = len(data_loader)\n",
    "# print(iter_per_epoch)#600\n",
    "total_step = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [100/10000], Loss: 2.2068, Acc: 0.44\n",
      "Step [200/10000], Loss: 2.1133, Acc: 0.62\n",
      "Step [300/10000], Loss: 1.9831, Acc: 0.67\n",
      "Step [400/10000], Loss: 1.8432, Acc: 0.75\n",
      "Step [500/10000], Loss: 1.6672, Acc: 0.84\n",
      "Step [600/10000], Loss: 1.6284, Acc: 0.76\n",
      "Step [700/10000], Loss: 1.4899, Acc: 0.88\n",
      "Step [800/10000], Loss: 1.3699, Acc: 0.81\n",
      "Step [900/10000], Loss: 1.2305, Acc: 0.81\n",
      "Step [1000/10000], Loss: 1.2140, Acc: 0.79\n",
      "Step [1100/10000], Loss: 1.0486, Acc: 0.87\n",
      "Step [1200/10000], Loss: 1.0591, Acc: 0.79\n",
      "Step [1300/10000], Loss: 0.9131, Acc: 0.83\n",
      "Step [1400/10000], Loss: 0.9403, Acc: 0.83\n",
      "Step [1500/10000], Loss: 0.8940, Acc: 0.82\n",
      "Step [1600/10000], Loss: 0.8860, Acc: 0.80\n",
      "Step [1700/10000], Loss: 0.7919, Acc: 0.84\n",
      "Step [1800/10000], Loss: 0.7651, Acc: 0.86\n",
      "Step [1900/10000], Loss: 0.7044, Acc: 0.86\n",
      "Step [2000/10000], Loss: 0.6755, Acc: 0.88\n",
      "Step [2100/10000], Loss: 0.6063, Acc: 0.87\n",
      "Step [2200/10000], Loss: 0.6927, Acc: 0.83\n",
      "Step [2300/10000], Loss: 0.5720, Acc: 0.90\n",
      "Step [2400/10000], Loss: 0.5194, Acc: 0.92\n",
      "Step [2500/10000], Loss: 0.5864, Acc: 0.88\n",
      "Step [2600/10000], Loss: 0.5769, Acc: 0.87\n",
      "Step [2700/10000], Loss: 0.6280, Acc: 0.87\n",
      "Step [2800/10000], Loss: 0.6060, Acc: 0.86\n",
      "Step [2900/10000], Loss: 0.5330, Acc: 0.90\n",
      "Step [3000/10000], Loss: 0.4836, Acc: 0.89\n",
      "Step [3100/10000], Loss: 0.5462, Acc: 0.86\n",
      "Step [3200/10000], Loss: 0.4702, Acc: 0.93\n",
      "Step [3300/10000], Loss: 0.4192, Acc: 0.92\n",
      "Step [3400/10000], Loss: 0.3914, Acc: 0.91\n",
      "Step [3500/10000], Loss: 0.4925, Acc: 0.90\n",
      "Step [3600/10000], Loss: 0.3781, Acc: 0.94\n",
      "Step [3700/10000], Loss: 0.4771, Acc: 0.88\n",
      "Step [3800/10000], Loss: 0.3863, Acc: 0.91\n",
      "Step [3900/10000], Loss: 0.3996, Acc: 0.87\n",
      "Step [4000/10000], Loss: 0.4182, Acc: 0.93\n",
      "Step [4100/10000], Loss: 0.4770, Acc: 0.85\n",
      "Step [4200/10000], Loss: 0.4545, Acc: 0.88\n",
      "Step [4300/10000], Loss: 0.4512, Acc: 0.87\n",
      "Step [4400/10000], Loss: 0.3376, Acc: 0.92\n",
      "Step [4500/10000], Loss: 0.3691, Acc: 0.89\n",
      "Step [4600/10000], Loss: 0.5728, Acc: 0.80\n",
      "Step [4700/10000], Loss: 0.5333, Acc: 0.86\n",
      "Step [4800/10000], Loss: 0.3460, Acc: 0.93\n",
      "Step [4900/10000], Loss: 0.3389, Acc: 0.91\n",
      "Step [5000/10000], Loss: 0.2938, Acc: 0.94\n",
      "Step [5100/10000], Loss: 0.3447, Acc: 0.93\n",
      "Step [5200/10000], Loss: 0.3108, Acc: 0.91\n",
      "Step [5300/10000], Loss: 0.2979, Acc: 0.94\n",
      "Step [5400/10000], Loss: 0.3308, Acc: 0.91\n",
      "Step [5500/10000], Loss: 0.3370, Acc: 0.93\n",
      "Step [5600/10000], Loss: 0.4292, Acc: 0.87\n",
      "Step [5700/10000], Loss: 0.3940, Acc: 0.88\n",
      "Step [5800/10000], Loss: 0.4349, Acc: 0.87\n",
      "Step [5900/10000], Loss: 0.3960, Acc: 0.92\n",
      "Step [6000/10000], Loss: 0.2652, Acc: 0.95\n",
      "Step [6100/10000], Loss: 0.2879, Acc: 0.94\n",
      "Step [6200/10000], Loss: 0.3092, Acc: 0.93\n",
      "Step [6300/10000], Loss: 0.3377, Acc: 0.94\n",
      "Step [6400/10000], Loss: 0.3435, Acc: 0.93\n",
      "Step [6500/10000], Loss: 0.3696, Acc: 0.91\n",
      "Step [6600/10000], Loss: 0.4338, Acc: 0.91\n",
      "Step [6700/10000], Loss: 0.2504, Acc: 0.94\n",
      "Step [6800/10000], Loss: 0.2754, Acc: 0.93\n",
      "Step [6900/10000], Loss: 0.3875, Acc: 0.91\n",
      "Step [7000/10000], Loss: 0.2749, Acc: 0.90\n",
      "Step [7100/10000], Loss: 0.3740, Acc: 0.92\n",
      "Step [7200/10000], Loss: 0.3603, Acc: 0.91\n",
      "Step [7300/10000], Loss: 0.3722, Acc: 0.90\n",
      "Step [7400/10000], Loss: 0.3270, Acc: 0.92\n",
      "Step [7500/10000], Loss: 0.4108, Acc: 0.90\n",
      "Step [7600/10000], Loss: 0.2992, Acc: 0.93\n",
      "Step [7700/10000], Loss: 0.2706, Acc: 0.90\n",
      "Step [7800/10000], Loss: 0.3759, Acc: 0.89\n",
      "Step [7900/10000], Loss: 0.3484, Acc: 0.91\n",
      "Step [8000/10000], Loss: 0.4274, Acc: 0.87\n",
      "Step [8100/10000], Loss: 0.2618, Acc: 0.94\n",
      "Step [8200/10000], Loss: 0.1616, Acc: 0.98\n",
      "Step [8300/10000], Loss: 0.2159, Acc: 0.93\n",
      "Step [8400/10000], Loss: 0.4070, Acc: 0.88\n",
      "Step [8500/10000], Loss: 0.3441, Acc: 0.89\n",
      "Step [8600/10000], Loss: 0.3980, Acc: 0.83\n",
      "Step [8700/10000], Loss: 0.2278, Acc: 0.94\n",
      "Step [8800/10000], Loss: 0.2775, Acc: 0.94\n",
      "Step [8900/10000], Loss: 0.3039, Acc: 0.90\n",
      "Step [9000/10000], Loss: 0.2902, Acc: 0.94\n",
      "Step [9100/10000], Loss: 0.4260, Acc: 0.90\n",
      "Step [9200/10000], Loss: 0.2569, Acc: 0.95\n",
      "Step [9300/10000], Loss: 0.2846, Acc: 0.88\n",
      "Step [9400/10000], Loss: 0.3814, Acc: 0.86\n",
      "Step [9500/10000], Loss: 0.3189, Acc: 0.91\n",
      "Step [9600/10000], Loss: 0.1350, Acc: 0.99\n",
      "Step [9700/10000], Loss: 0.2868, Acc: 0.92\n",
      "Step [9800/10000], Loss: 0.3878, Acc: 0.89\n",
      "Step [9900/10000], Loss: 0.2627, Acc: 0.92\n",
      "Step [10000/10000], Loss: 0.3234, Acc: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "for step in range(total_step):\n",
    "    \n",
    "    # Reset the data_iter\n",
    "    if (step+1) % iter_per_epoch == 0:\n",
    "        data_iter = iter(data_loader)\n",
    "        #print(data_iter)\n",
    "    \n",
    "#     print(step)\n",
    "    # Fetch images and labels\n",
    "    images, labels = next(data_iter)\n",
    "    images, labels = images.view(images.size(0), -1).to(device), labels.to(device)\n",
    "#     print(labels.size())#torch.Size([100])\n",
    "    \n",
    "    # Forward pass\n",
    "#     print(images.size())#torch.Size([100, 784])\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)#计算损失\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute accuracy\n",
    "    _, argmax = torch.max(outputs, 1)\n",
    "    accuracy = (labels == argmax.squeeze()).float().mean()\n",
    "\n",
    "    if (step+1) % 100 == 0:\n",
    "        print ('Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
    "               .format(step+1, total_step, loss.item(), accuracy.item()))\n",
    "\n",
    "        # ================================================================== #\n",
    "        #                        Tensorboard Logging                         #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # 1. Log scalar values (scalar summary)\n",
    "        info = { 'loss': loss.item(), 'accuracy': accuracy.item() }\n",
    "\n",
    "        for tag, value in info.items():\n",
    "            logger.scalar_summary(tag, value, step+1)\n",
    "\n",
    "        # 2. Log values and gradients of the parameters (histogram summary)\n",
    "        for tag, value in model.named_parameters():\n",
    "            tag = tag.replace('.', '/')\n",
    "            logger.histo_summary(tag, value.data.cpu().numpy(), step+1)\n",
    "            logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), step+1)\n",
    "\n",
    "        # 3. Log training images (image summary)\n",
    "        info = { 'images': images.view(-1, 28, 28)[:10].cpu().numpy() }\n",
    "\n",
    "        for tag, images in info.items():\n",
    "            logger.image_summary(tag, images, step+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
